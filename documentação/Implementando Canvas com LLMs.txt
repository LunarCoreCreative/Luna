Arquitetura Avançada e Implementação de Interfaces Generativas do Tipo Canvas: Um Guia Definitivo para Engenharia de Agentes com Together AI e Vercel AI SDK
1. Introdução: A Mudança de Paradigma na Interação Humano-IA
A evolução das interfaces de usuário baseadas em inteligência artificial (GenUI) atingiu um ponto de inflexão crítico. A primeira geração de assistentes baseados em Grandes Modelos de Linguagem (LLMs), exemplificada pelo ChatGPT original, baseava-se fundamentalmente no paradigma do "Chat Linear". Neste modelo, a interação é efêmera, sequencial e baseada em texto corrido, mimetizando a conversação humana. No entanto, à medida que a utilização da IA transita de tarefas simples de recuperação de informação para trabalhos cognitivos complexos — como engenharia de software, redação técnica e análise de dados estruturados — as limitações do chat linear tornam-se evidentes. O fluxo conversacional não oferece persistência de estado, controle de versão ou a visualização espacial necessária para a co-criação iterativa.
Em resposta a estas limitações, surgiu o paradigma do "Canvas" (ou "Artifacts", na terminologia da Anthropic). Esta arquitetura introduz uma bifurcação na experiência do usuário, separando a interação em dois fluxos distintos: o fluxo epistêmico (a conversa sobre o trabalho) e o fluxo ôntico (o trabalho em si, manifestado como um artefato persistente e editável). Implementações notáveis como o OpenAI Canvas e o Anthropic Artifacts demonstraram que esta separação reduz a carga cognitiva do usuário e aumenta a precisão do modelo ao isolar o contexto de geração.1
Este relatório técnico oferece uma análise exaustiva e um roteiro de implementação para arquitetar um sistema similar ao Canvas para um assistente personalizado ("Luna"). O foco central recai sobre a resolução do problema de "vazamento de chat" — a falha do modelo em invocar ferramentas adequadas, revertendo para a geração de código diretamente no fluxo de conversação — utilizando a infraestrutura de inferência da Together AI e o framework de aplicação Vercel AI SDK. A análise aprofunda-se na engenharia de prompts de sistema, na definição rigorosa de esquemas de ferramentas e na gestão de estado no frontend, fornecendo uma base sólida para a construção de agentes de co-criação robustos.
1.1 O Desafio da Aderência ao Uso de Ferramentas ("Tool Adherence")
O problema relatado com a assistente "Luna" — ignorar a instrução de usar a ferramenta de canvas e escrever código diretamente no chat — não é um erro trivial de programação, mas sim um reflexo fundamental do alinhamento dos LLMs modernos. A grande maioria dos modelos, incluindo as famílias Llama e Qwen disponíveis na Together AI, foi treinada com Reinforcement Learning from Human Feedback (RLHF) para priorizar respostas úteis e diretas em formato de texto. Quando um usuário solicita "crie um componente React", a distribuição de probabilidade do modelo favorece fortemente a geração imediata dos tokens de código, pois foi assim que ele foi recompensado durante o treinamento.3
Para reverter esse comportamento e impor uma arquitetura baseada em ferramentas, é necessário criar uma "dissonância artificial" no contexto do modelo. O sistema deve ser projetado de tal forma que a geração de texto livre seja penalizada ou estruturalmente impossível para tarefas de criação, obrigando o modelo a recorrer às ferramentas (Function Calling) como único meio viável de cumprir a solicitação do usuário. Isso exige uma orquestração precisa entre o prompt do sistema (o "cérebro"), a definição da API da ferramenta (os "membros") e o loop de feedback da aplicação (o "ambiente").
1.2 O Ecossistema Tecnológico: Together AI e Vercel AI SDK
A escolha da pilha tecnológica é determinante para o sucesso desta arquitetura. A Together AI posiciona-se como um provedor de inferência de alto desempenho, especializado em modelos de código aberto como o Llama 3.1 e o Qwen 2.5. A capacidade destes modelos de seguir instruções complexas e esquemas JSON estritos é vital para a funcionalidade do Canvas. O modelo Llama 3.1 70B Instruct Turbo, em particular, oferece um equilíbrio ideal entre raciocínio lógico e latência, aproximando-se do desempenho de modelos proprietários de fronteira.5
Complementarmente, o Vercel AI SDK fornece a camada de abstração necessária para gerenciar o fluxo de dados (streaming). A sua capacidade de lidar com chamadas de ferramentas no lado do servidor e renderizar interfaces generativas no lado do cliente (via React Server Components ou Hooks) simplifica drasticamente a complexidade de engenharia necessária para sincronizar o estado do chat com o estado do artefato.7
2. Fundamentos Teóricos da Arquitetura Canvas
Antes de adentrar a implementação de código, é imperativo estabelecer o modelo mental correto para um sistema de Canvas. Ao contrário de um chatbot tradicional, que opera em um ciclo simples de Request -> Response, um sistema Canvas opera como uma máquina de estados complexa onde o LLM atua como um controlador inteligente de transições de estado.
2.1 A Dicotomia Epistêmica e Ôntica na Interface
A teoria da interface de usuário para IA generativa identifica dois modos de operação que devem coexistir, mas não se misturar:
* O Modo Epistêmico (Conversação): Este modo lida com a intenção, a ambiguidade, o esclarecimento e o feedback. É o espaço onde o usuário diz "Mude a cor para azul" ou "Explique este código". A persistência aqui é linear e histórica; o valor de uma mensagem diminui à medida que ela recua no histórico.
* O Modo Ôntico (Artefato): Este modo lida com o objeto da criação. É o espaço onde o código reside, onde o documento é editado. A persistência aqui é baseada em estado e versão; o valor do artefato é absoluto e atual.
A falha de "Luna" representa um colapso desta dicotomia: ela está tentando realizar uma tarefa ôntica (criar código) dentro do canal epistêmico (o chat). A arquitetura deve impor barreiras rígidas para prevenir esse vazamento. A documentação da OpenAI sobre o Canvas e da Anthropic sobre os Artifacts sugere que essa separação é forçada tanto por heurísticas de pré-processamento quanto por instruções sistêmicas draconianas.8
2.2 O Ciclo de Vida da Geração de Artefatos
O fluxo de dados em uma interação de Canvas bem-sucedida segue um padrão específico, radicalmente diferente de um chat padrão:
1. Detecção de Intenção: O sistema analisa o prompt do usuário. Se o prompt implica a criação de conteúdo substancial (código, texto longo, diagrama), o sistema ativa o modo de "Criação".
2. Roteamento de Ferramenta: O modelo é instruído ou forçado a não gerar texto, mas sim invocar uma ferramenta específica, por exemplo, generate_artifact.
3. Execução e Streaming: A saída da ferramenta não é um bloco de texto estático, mas um fluxo de dados estruturados. O Vercel AI SDK intercepta este fluxo.
4. Renderização Condicional: O frontend detecta a assinatura da chamada de ferramenta e, em vez de renderizar um balão de chat, instancia ou atualiza o componente visual do Canvas (o painel lateral).
5. Sincronização de Estado: O conteúdo gerado torna-se o novo "estado da verdade" do artefato, desvinculado do histórico linear do chat, permitindo edições futuras sem a necessidade de reprocessar toda a conversa anterior.
2.3 Estratégias de Mutação de Estado: Rewrite vs. Diff
Uma decisão arquitetural crucial é como o modelo deve atualizar o artefato existente. As pesquisas indicam duas abordagens predominantes, cada uma com implicações profundas sobre a confiabilidade e o custo (tokens).10
Estratégia
	Descrição
	Vantagens
	Desvantagens
	Full Rewrite (Reescrita Total)
	O modelo envia o conteúdo completo do arquivo a cada alteração.
	Confiabilidade Máxima: O modelo tem visão total do contexto; elimina erros de aplicação de patch. Simplicidade: O frontend apenas substitui a string.
	Alta Latência: O usuário espera a geração de 500 linhas para mudar uma vírgula. Custo Elevado: Consumo excessivo de tokens de saída.
	Search & Replace (Busca e Substituição)
	O modelo emite blocos definindo o texto a buscar e o texto substituto.
	Eficiência: Apenas as linhas alteradas são geradas. Latência muito baixa para pequenas edições.
	Fragilidade Extrema: Se o modelo "alucinar" um espaço em branco no bloco de busca, a aplicação falha. Exige modelos muito precisos.
	Unified Diff
	O modelo gera um diff no formato padrão do Git (+ / -).
	Padronização: Aproveita bibliotecas de parsing existentes.
	Dificuldade Cognitiva: Modelos de linguagem lutam com a contagem precisa de linhas e contexto, resultando em diffs corrompidos.
	Component-Based Update
	O artefato é modular (ex: componentes React individuais). O modelo atualiza apenas o módulo alvo.
	Equilíbrio: Combina a segurança da reescrita com a eficiência da granularidade.
	Complexidade de Orquestração: Exige um gerenciador de estado que entenda a estrutura modular do artefato.
	Para a implementação inicial na Luna, recomenda-se a estratégia de Full Rewrite. Embora mais custosa, ela elimina uma classe inteira de erros (falhas de merge/patch) que poderiam confundir o diagnóstico de problemas de aderência à ferramenta. A otimização para diffs deve ser um passo secundário, implementado apenas após o modelo estar utilizando as ferramentas de forma consistente.
3. Engenharia do "Cérebro": O Prompt do Sistema (System Prompt)
A análise forense de prompts "vazados" de sistemas avançados como o Claude Artifacts e o Bolt.new revela que a instrução do sistema é o componente mais crítico para o sucesso desta arquitetura. Um prompt genérico como "Você é um assistente útil" é insuficiente para superar o viés de treinamento do modelo em direção ao chat.9 O prompt deve funcionar como um "kernel" de sistema operacional, definindo as leis físicas do ambiente da IA.
3.1 Anatomia de um Prompt de Sistema Agentico
Para corrigir o comportamento da Luna, o prompt do sistema deve ser reestruturado para incluir restrições explícitas, definições de ambiente e gatilhos de pensamento (Chain of Thought).
3.1.1 Definição de Identidade e Restrições Operacionais
É necessário estabelecer uma identidade que não seja apenas "conversacional", mas "operacional". O prompt deve proibir explicitamente a saída de código no chat.
Análise do Prompt do Bolt.new 12:
O sistema Bolt utiliza tags XML como <system_constraints> para delimitar regras invioláveis. Ele define o ambiente como um "WebContainer" e proíbe terminantemente a verbosidade. O modelo é instruído a agir como um desenvolvedor sênior que se comunica principalmente através de código estruturado.
Recomendação para a Luna:
O prompt deve conter uma diretiva negativa forte.
* Errado: "Por favor, use a ferramenta de artefato para código."
* Correto: "Você ESTÁ ESTRITAMENTE PROIBIDO de gerar blocos de código ou documentos longos diretamente na resposta do chat. Todo conteúdo substancial DEVE ser encapsulado através da chamada da ferramenta create_artifact. A violação desta regra resultará em falha crítica do sistema."
3.1.2 Cadeia de Pensamento (Chain of Thought) Pré-Ferramenta
Modelos como o Llama 3.1 beneficiam-se enormemente da capacidade de "pensar" antes de agir. Instruir o modelo a gerar um bloco de pensamento <thought> antes de chamar a ferramenta aumenta a aderência, pois permite que o modelo planeje a ação.14
Estratégia de Implementação:
Instrua a Luna a seguir este fluxo lógico:
1. Receber input do usuário.
2. Gerar um token de pensamento interno: "O usuário pediu um script Python. Isso é um conteúdo substancial? Sim."
3. Decidir invocar a ferramenta com base nessa conclusão.
4. Emitir a chamada da ferramenta.
3.2 Estrutura e Tipagem de Artefatos
A análise dos prompts da Anthropic 16 mostra o uso de tipos MIME específicos e identificadores para gerenciar artefatos. Isso ajuda o modelo a entender o formato esperado.
Tipos Recomendados para o Prompt da Luna:
* application/vnd.ant.code: Para código fonte (Python, JS, etc.).
* text/markdown: Para documentos, relatórios e ensaios.
* image/svg+xml: Para diagramas vetoriais e visualizações.
* application/vnd.ant.react: Para componentes de UI interativos (usando Tailwind CSS).
Ao definir estes tipos no prompt do sistema, cria-se um contrato claro: "Se você vai gerar React, deve usar este tipo específico na ferramenta". Isso reduz a ambiguidade que leva ao vazamento de chat.
3.3 Protocolo de Prevenção de Vazamento
O vazamento ocorre frequentemente quando o modelo interpreta a solicitação como uma pergunta ("Como faço X?") em vez de uma ordem ("Faça X"). O prompt deve reformular a interpretação.
Instrução de Reenquadramento:
"Sua função não é ensinar o usuário a codificar através do chat, mas sim entregar soluções prontas no Canvas. Se a resposta envolver código executável, a criação do artefato é mandatória, não opcional."
4. Implementação de Backend: Together AI e Definição de Ferramentas
Esta seção detalha a implementação técnica utilizando a API da Together AI. A Together AI oferece compatibilidade com a biblioteca cliente da OpenAI, o que facilita a integração com o Vercel AI SDK, mas existem nuances específicas na configuração dos modelos Llama para uso eficaz de ferramentas (Tool Calling).18
4.1 Configuração do Provedor Together AI
Para utilizar os modelos da Together AI com o Vercel AI SDK, utiliza-se o adaptador createOpenAI, redirecionando a baseURL. Isso permite aproveitar a infraestrutura de streaming e tipagem do SDK enquanto se utiliza a inferência econômica e rápida da Together.


TypeScript




import { createOpenAI } from '@ai-sdk/openai';

// Configuração do provedor Together AI compatível com OpenAI
export const together = createOpenAI({
 apiKey: process.env.TOGETHER_API_KEY,
 baseURL: 'https://api.together.xyz/v1', // Endpoint oficial compatível [19]
});

// Seleção do Modelo: Llama 3.1 70B Instruct Turbo
// Este modelo é especificamente otimizado para tool calling e raciocínio complexo.
export const model = together('meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo');

Insight Arquitetural: A escolha do modelo Meta-Llama-3.1-70B-Instruct-Turbo não é acidental. Modelos menores (8B) tendem a ignorar instruções de ferramentas com mais frequência, enquanto o 405B pode ser excessivamente lento e custoso para atualizações em tempo real de canvas. O 70B representa o "sweet spot" atual para agentes de codificação.5
4.2 Definição Rigorosa do Esquema da Ferramenta (Zod)
Para garantir que a Luna utilize a ferramenta corretamente, a definição da ferramenta deve ser inequívoca. O uso de esquemas Zod (z.object) permite que o Vercel AI SDK valide a saída do modelo e garante que a estrutura JSON gerada esteja correta.20


TypeScript




import { z } from 'zod';
import { tool } from 'ai';

export const tools = {
 create_artifact: tool({
   description: 'CRÍTICO: Use esta ferramenta SEMPRE que precisar gerar código, documentos ou diagramas. NÃO escreva código no chat.',
   parameters: z.object({
     title: z.string().describe('Um título descritivo para o artefato (ex: "Componente de Login", "Análise de Vendas")'),
     type: z.enum(['code', 'markdown', 'svg', 'mermaid', 'react'])
      .describe('O tipo MIME ou categoria do conteúdo. Use "react" para interfaces web.'),
     language: z.string().optional()
      .describe('A linguagem de programação (ex: python, typescript) para artefatos de código.'),
     content: z.string()
      .describe('O conteúdo COMPLETO e executável do artefato. Não use placeholders ou comentários como "// resto do código".'),
   }),
   execute: async ({ title, type, content }) => {
     // No lado do servidor, esta função pode salvar o artefato no banco de dados.
     // Para o Canvas, o retorno é enviado ao cliente para renderização.
     return { 
       id: crypto.randomUUID(),
       status: 'generated',
       title,
       type 
     };
   },
 }),
};

4.3 A Solução "Força Bruta": Tool Choice Obrigatório
Se a Luna continuar resistindo ao uso da ferramenta, a API permite forçar o comportamento. Podemos implementar uma heurística no backend que analisa a intenção do usuário e, se detectar palavras-chave de criação, define o parâmetro toolChoice como required.21
Estratégia de Implementação no Route Handler:


TypeScript




// app/api/chat/route.ts
import { streamText, convertToModelMessages } from 'ai';
import { model } from '@/lib/ai/together';
import { tools } from '@/lib/ai/tools';

export async function POST(req: Request) {
 const { messages } = await req.json();
 
 // Análise heurística da última mensagem
 const lastMessage = messages[messages.length - 1].content.toLowerCase();
 const creationKeywords = ['criar', 'gerar', 'escrever código', 'fazer um site', 'desenhar'];
 const userIntendsCreation = creationKeywords.some(keyword => lastMessage.includes(keyword));

 const result = await streamText({
   model: model,
   messages: convertToModelMessages(messages),
   tools: tools,
   // Se a intenção for clara, OBRIGA o modelo a usar a ferramenta.
   // Caso contrário, deixa no automático.
   toolChoice: userIntendsCreation? 'required' : 'auto', 
   system: `... (Inserir Prompt de Sistema Robusto aqui)...`,
 });

 return result.toDataStreamResponse();
}

Esta técnica atua como um "guardrail" (guarda-corpo) programático, garantindo que mesmo que o modelo hesite, a API o force a entrar no modo de estruturação de dados.
5. Arquitetura de Frontend e Integração com Vercel AI SDK
O frontend é onde a mágica do Canvas acontece. A complexidade aqui reside em interceptar o fluxo de dados que contém tanto texto (chat) quanto chamadas de função (artefato) e renderizá-los em painéis separados.
5.1 O Hook useChat e a Gestão de Estado
O Vercel AI SDK fornece o hook useChat, que gerencia automaticamente o estado das mensagens e das chamadas de ferramentas. A chave para a implementação do Canvas é observar a propriedade toolInvocations nas mensagens.23
Ao contrário de abordagens mais antigas que esperavam a resposta completa, o SDK moderno suporta streaming de chamadas de ferramentas. Isso significa que podemos começar a renderizar o código no Canvas enquanto ele ainda está sendo gerado, proporcionando um feedback visual instantâneo e "mágico" para o usuário.
Lógica de Renderização do Componente Principal:


TypeScript




// components/interface-principal.tsx
'use client';
import { useChat } from '@ai-sdk/react';
import { CanvasPanel } from './canvas-panel';
import { ChatPanel } from './chat-panel';
import { useEffect, useState } from 'react';

export function InterfacePrincipal() {
 const { messages, input, handleInputChange, handleSubmit } = useChat({
   api: '/api/chat',
   // Callback para efeitos colaterais quando uma ferramenta é chamada
   onToolCall: ({ toolCall }) => {
     if (toolCall.toolName === 'create_artifact') {
       setIsCanvasOpen(true); // Abre o painel automaticamente
     }
   },
 });

 const [activeArtifact, setActiveArtifact] = useState(null);

 // Efeito para sincronizar o último artefato gerado com o estado do Canvas
 useEffect(() => {
   const lastMessage = messages[messages.length - 1];
   if (lastMessage?.toolInvocations) {
     const artifactCall = lastMessage.toolInvocations.find(
       t => t.toolName === 'create_artifact'
     );
     if (artifactCall && 'args' in artifactCall) {
       // Atualiza o artefato em tempo real com os argumentos parciais do stream
       setActiveArtifact(artifactCall.args);
     }
   }
 }, [messages]);

 return (
   <div className="flex h-screen w-full bg-background">
     <div className="w-1/3 border-r">
       <ChatPanel 
         messages={messages} 
         input={input} 
         handleInputChange={handleInputChange} 
         handleSubmit={handleSubmit} 
       />
     </div>
     <div className="w-2/3">
       <CanvasPanel artifact={activeArtifact} />
     </div>
   </div>
 );
}

5.2 Renderização Polimórfica de Artefatos
O componente CanvasPanel deve ser capaz de renderizar diferentes tipos de conteúdo com base no parâmetro type retornado pela ferramenta. Esta flexibilidade é o que distingue um Canvas real de um simples visualizador de código.25
* Código (React/HTML): Para renderizar interfaces web seguras, recomenda-se o uso de sandpack (da CodeSandbox) ou iframes isolados. Isso permite a execução de código React gerado pela IA sem risco de segurança para a aplicação principal.
* Markdown: Utilização de bibliotecas como react-markdown com suporte a GFM (GitHub Flavored Markdown) para renderizar documentação rica.
* Mermaid/Diagramas: Renderização dinâmica de gráficos e fluxogramas utilizando mermaid.js, permitindo que a IA desenhe arquiteturas de sistema visualmente.
5.3 O Dilema streamText vs streamUI
O Vercel AI SDK introduziu recentemente o streamUI (baseado em React Server Components - RSC), que permite transmitir componentes de UI diretamente do servidor. No entanto, para a arquitetura de Canvas, o método streamText com manipulação de ferramentas no cliente é frequentemente superior e mais flexível.27
Análise Comparativa:
* streamUI: Ideal para interfaces generativas efêmeras (ex: um widget de clima que aparece no chat). Difícil de persistir estado complexo de edição ou permitir interação do usuário (como editar o código gerado).
* streamText + Tool Calling (Recomendado): Separa os dados (o código do artefato) da apresentação. O cliente recebe os dados brutos e tem controle total sobre como renderizá-los (no editor Monaco, num preview, etc.). Isso facilita a implementação de funcionalidades de edição manual pelo usuário, algo essencial num Canvas.
6. Otimizações para Modelos Together AI
A infraestrutura da Together AI possui características específicas que podem ser exploradas para otimizar a experiência da Luna.
6.1 Modo JSON vs. Tool Calling Nativo
A Together AI suporta um modo JSON (response_format: { type: "json_object" }) que garante que a saída seja um JSON válido. No entanto, para o Llama 3.1, o uso da API nativa de ferramentas (que utiliza tokens especiais como <|python_tag|> ou tokens de controle de ferramenta específicos do modelo) é preferível. O modelo foi ajustado ("fine-tuned") para reagir a esses tokens de forma mais robusta do que a instruções puras de formatação JSON no prompt.5
Recomendação: Utilize a abstração de ferramentas do Vercel AI SDK, que mapeia automaticamente para a API de ferramentas nativa da Together, em vez de tentar forçar um JSON manual via prompt.
6.2 Gestão de Janela de Contexto (Context Window)
O Llama 3.1 70B possui uma janela de contexto de 128k tokens. Embora generosa, enviar o histórico completo de edições de um artefato grande (ex: um arquivo de 2000 linhas sendo editado 10 vezes) consumirá essa janela rapidamente e aumentará a latência e o custo.3
Estratégia de Compressão:
Não mantenha o conteúdo total de todas as versões anteriores do artefato no histórico de mensagens enviado ao modelo.
1. Quando um artefato é criado, salve seu conteúdo no estado do cliente/banco de dados.
2. No histórico de mensagens enviado para a próxima inferência, substitua o conteúdo do corpo da chamada da ferramenta antiga por um placeholder ou resumo: [Conteúdo do artefato 'app.tsx' ocultado - 1500 linhas].
3. Injete apenas o estado atual do artefato no System Prompt ou como uma mensagem de contexto recente, garantindo que o modelo veja apenas o que é relevante para a próxima edição.
6.3 Parâmetros de Inferência
A "criatividade" é inimiga da estrutura em tarefas de Canvas. Para reduzir a alucinação de comandos e aumentar a adesão à sintaxe:
* Temperature: Ajustar para baixo (0.1 - 0.2). Temperaturas altas encorajam o modelo a "improvisar", o que frequentemente resulta em ignorar a ferramenta e escrever no chat.
* Top_P: Manter restritivo (0.9 ou menos) para cortar a cauda de distribuições de tokens improváveis.
7. Estratégias de Mitigação de Falhas ("Troubleshooting Luna")
Se, após todas as implementações acima, a Luna ainda apresentar comportamentos indesejados, deve-se proceder com uma análise diagnóstica estruturada.
7.1 Diagnóstico: O Modelo "Pede Desculpas" e Não Gera Nada
Isso geralmente indica um disparo dos filtros de segurança (Safety Rails). O Llama 3.1 tem alinhamento de segurança forte. Se o usuário pedir "hackear um site", o modelo recusará. No entanto, às vezes ele recusa código inócuo se interpretar o ambiente como inseguro.
* Solução: Reforçar no prompt do sistema que o ambiente é um "Sandboxed WebContainer" seguro e destinado ao desenvolvimento educacional e profissional.
7.2 Diagnóstico: O Modelo Gera a Chamada da Ferramenta mas o JSON é Inválido
Isso ocorre em streaming quando a conexão cai ou o modelo é interrompido.
* Solução: Implementar um parser de JSON resiliente no frontend (como best-effort-json-parser) que consiga ler estruturas parciais, ou utilizar a funcionalidade nativa do SDK toolInvocations que já trata o parsing incremental.31
7.3 Diagnóstico: Vazamento Parcial (Texto + Ferramenta)
O modelo escreve "Aqui está o código:" no chat e também chama a ferramenta. Isso suja a interface.
* Solução (Frontend): Implementar um "Limpador de Chat". Se uma mensagem contiver uma invocação de ferramenta, o componente de chat pode ocultar automaticamente qualquer texto que a preceda, ou renderizar apenas um "Badge" indicando "Gerando artefato...", ignorando o texto explicativo redundante.
8. Conclusão e Roteiro Estratégico
A implementação da funcionalidade de Canvas na assistente "Luna" não é apenas uma tarefa de integração de API, mas um exercício de orquestração de comportamento de IA. O problema de a assistente escrever no chat em vez de usar as ferramentas é sintomático de um conflito entre o treinamento do modelo e a arquitetura da aplicação.
Para resolver isso definitivamente, a solução exige:
1. Engenharia de Prompt Autoritária: Substituir a polidez por restrições operacionais rígidas e XML estruturado.
2. Infraestrutura Together AI: Utilizar o modelo Llama 3.1 70B com configurações de temperatura baixa e, quando necessário, forçar a escolha da ferramenta (tool_choice: required).
3. Frontend Reativo: Adotar o Vercel AI SDK para gerenciar o streaming de ferramentas, desacoplando visualmente a criação do artefato da conversa linear.
Ao transformar a Luna de uma parceira de conversação em uma operadora de ferramentas, elevamos a experiência do usuário de uma simples troca de mensagens para uma verdadeira plataforma de engenharia assistida por IA.
Tabela Comparativa de Abordagens de Integração
Componente
	Abordagem Tradicional (Chatbot)
	Abordagem Canvas (Agentica)
	Benefício para Luna
	Prompt do Sistema
	"Você é útil e amigável."
	"Você é um motor de execução operando em um WebContainer. Use ferramentas para output."
	Impede o vazamento de código no chat.
	Formato de Saída
	Texto Markdown (```).
	Chamada de Ferramenta JSON Estruturado.
	Permite renderização rica e edição.
	Gestão de Estado
	Histórico de mensagens linear.
	Estado do Artefato separado do Histórico.
	Permite iteração sem poluir o contexto.
	Streaming
	Texto token-a-token.
	Streaming de dados estruturados/deltas.
	Feedback visual imediato no painel lateral.
	Modelo (Together AI)
	Llama 3.1 8B (Rápido/Genérico).
	Llama 3.1 70B (Raciocínio/Ferramentas).
	Maior aderência a instruções complexas.
	Este roteiro fornece a base técnica necessária para transformar a funcionalidade da Luna, alinhando-a com o estado da arte em interfaces generativas.
Referências citadas
1. What is the canvas feature in ChatGPT and how do I use it? - OpenAI Help Center, acessado em dezembro 31, 2025, https://help.openai.com/en/articles/9930697-what-is-the-canvas-feature-in-chatgpt-and-how-do-i-use-it
2. Anthropic just dropped Claude Artifacts - now you can build AI powered apps in your browser. Here's what you can do with it, the most popular use cases and what most people don't know about it : r/ThinkingDeeplyAI - Reddit, acessado em dezembro 31, 2025, https://www.reddit.com/r/ThinkingDeeplyAI/comments/1lp4yo2/anthropic_just_dropped_claude_artifacts_now_you/
3. Why Long System Prompts Hurt Context Windows (and How to Fix It) - Medium, acessado em dezembro 31, 2025, https://medium.com/data-science-collective/why-long-system-prompts-hurt-context-windows-and-how-to-fix-it-7a3696e1cdf9
4. Effective context engineering for AI agents - Anthropic, acessado em dezembro 31, 2025, https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents
5. Function Calling - Together.ai Docs, acessado em dezembro 31, 2025, https://docs.together.ai/docs/function-calling
6. Dedicated Endpoints - Together AI, acessado em dezembro 31, 2025, https://www.together.ai/dedicated-endpoints
7. AI SDK by Vercel, acessado em dezembro 31, 2025, https://ai-sdk.dev/docs/introduction
8. Introducing canvas, a new way to write and code with ChatGPT. | OpenAI, acessado em dezembro 31, 2025, https://openai.com/index/introducing-canvas/
9. Claude's Inner Monologue: System Prompt Leaked, Revealing the AI's Digital DNA, acessado em dezembro 31, 2025, https://medium.com/@lahsaini/claudes-inner-monologue-system-prompt-leaked-revealing-the-ai-s-digital-dna-fc4a10ec2612
10. AI Code Edit Formats Guide 2025: Diff vs Whole File vs Semantic - Morph, acessado em dezembro 31, 2025, https://morphllm.com/edit-formats
11. Code Surgery: How AI Assistants Make Precise Edits to Your Files - Fabian Hertwig's Blog, acessado em dezembro 31, 2025, https://fabianhertwig.com/blog/coding-assistants-file-edits/
12. bolt.new/app/lib/.server/llm/prompts.ts at main · stackblitz ... - GitHub, acessado em dezembro 31, 2025, https://github.com/stackblitz/bolt.new/blob/main/app/lib/.server/llm/prompts.ts
13. leaked-system-prompts/bolt.new_20241009.md at main - GitHub, acessado em dezembro 31, 2025, https://github.com/jujumilk3/leaked-system-prompts/blob/main/bolt.new_20241009.md
14. A forensic analysis of the Claude Sonnet 3.5 system prompt leak - DEV Community, acessado em dezembro 31, 2025, https://dev.to/ejb503/a-forensic-analysis-of-the-claude-sonnet-35-system-prompt-leak-58h7
15. Claude 3.5 Sonnet, Full Artifacts System Prompt - GitHub Gist, acessado em dezembro 31, 2025, https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3ecd
16. Leaked System Prompt Shows How Anthropic Guides Its LLM - HOLO, acessado em dezembro 31, 2025, https://www.holo.mg/stream/anthropic-claude-system-prompt-leaked/
17. The hidden Claude system prompt (on the Artefacts system, new response styles, thinking tags, and more...) : r/ClaudeAI - Reddit, acessado em dezembro 31, 2025, https://www.reddit.com/r/ClaudeAI/comments/1hb3evv/the_hidden_claude_system_prompt_on_the_artefacts/
18. Together.ai Provider - AI SDK, acessado em dezembro 31, 2025, https://ai-sdk.dev/providers/ai-sdk-providers/togetherai
19. OpenAI Compatibility - Together.ai Docs, acessado em dezembro 31, 2025, https://docs.together.ai/docs/openai-api-compatibility
20. AI SDK Core: Tool Calling, acessado em dezembro 31, 2025, https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling
21. Function Calling - xAI API, acessado em dezembro 31, 2025, https://docs.x.ai/docs/guides/function-calling
22. Making AI Useful: How to Use json_schema and Function (via. tools) in the Responses API, acessado em dezembro 31, 2025, https://medium.com/@arda.arslan/making-ai-useful-how-to-use-json-schema-and-function-via-tools-in-the-responses-api-a36568ab6694
23. Chatbot - AI SDK UI, acessado em dezembro 31, 2025, https://ai-sdk.dev/docs/ai-sdk-ui/chatbot
24. Chatbot Tool Usage - AI SDK UI, acessado em dezembro 31, 2025, https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage
25. Convert Claude Artefacts to React Next with a single Prompt - GitHub, acessado em dezembro 31, 2025, https://github.com/gbechtold/Claude-to-React-Next
26. Rendering & Streaming Artifacts - Thesys Documentation, acessado em dezembro 31, 2025, https://docs.thesys.dev/guides/artifacts/rendering
27. AI SDK 6 - Vercel, acessado em dezembro 31, 2025, https://vercel.com/blog/ai-sdk-6
28. Do you recommend streamUI or the streamobject, streamtext, etc · vercel ai · Discussion #2162 - GitHub, acessado em dezembro 31, 2025, https://github.com/vercel/ai/discussions/2162
29. The guide to structured outputs and function calling with LLMs - Agenta, acessado em dezembro 31, 2025, https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms
30. Disadvantage of Long Prompt for LLM - PromptLayer Blog, acessado em dezembro 31, 2025, https://blog.promptlayer.com/disadvantage-of-long-prompt-for-llm/
31. ocherry341/llm-xml-parser: A XML parser for output structured, streaming data from LLMs - GitHub, acessado em dezembro 31, 2025, https://github.com/ocherry341/llm-xml-parser
32. AI SDK 5 - Vercel, acessado em dezembro 31, 2025, https://vercel.com/blog/ai-sdk-5